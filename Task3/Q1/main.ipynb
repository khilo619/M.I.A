{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473334b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a3606f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "message_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_links",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "has_offer",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sender_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "all_caps",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is_spam",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9a1228c4-31f4-4d65-ac1a-0c0849c01d4e",
       "rows": [
        [
         "5399",
         "5400",
         "0",
         "101",
         "0",
         "0.7843839378864996",
         "0",
         "0"
        ],
        [
         "11346",
         "11347",
         "1",
         "164",
         "0",
         "0.4061443290409883",
         "0",
         "0"
        ],
        [
         "2675",
         "2676",
         "1",
         "184",
         "0",
         "0.8448191744415716",
         "0",
         "0"
        ],
        [
         "14413",
         "14414",
         "2",
         "97",
         "0",
         "0.683464243084368",
         "0",
         "0"
        ],
        [
         "1869",
         "1870",
         "4",
         "110",
         "0",
         "0.6275172941355043",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>num_links</th>\n",
       "      <th>num_words</th>\n",
       "      <th>has_offer</th>\n",
       "      <th>sender_score</th>\n",
       "      <th>all_caps</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>5400</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.784384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11346</th>\n",
       "      <td>11347</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.406144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>2676</td>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844819</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14413</th>\n",
       "      <td>14414</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0.683464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>1870</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id  num_links  num_words  has_offer  sender_score  all_caps  \\\n",
       "5399         5400          0        101          0      0.784384         0   \n",
       "11346       11347          1        164          0      0.406144         0   \n",
       "2675         2676          1        184          0      0.844819         0   \n",
       "14413       14414          2         97          0      0.683464         0   \n",
       "1869         1870          4        110          0      0.627517         0   \n",
       "\n",
       "       is_spam  \n",
       "5399         0  \n",
       "11346        0  \n",
       "2675         0  \n",
       "14413        0  \n",
       "1869         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7aeb1",
   "metadata": {},
   "source": [
    "**No need for the message_id col, won't help in classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40776205",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('message_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80eb73e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_links",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "has_offer",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sender_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "all_caps",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is_spam",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b4d51098-cf10-4b45-8030-ddf91e048a38",
       "rows": [
        [
         "0",
         "3",
         "98",
         "1",
         "0.718607000019532",
         "0",
         "0"
        ],
        [
         "1",
         "0",
         "170",
         "0",
         "0.6989012256305066",
         "1",
         "0"
        ],
        [
         "2",
         "0",
         "38",
         "0",
         "0.6204655338427096",
         "0",
         "0"
        ],
        [
         "3",
         "0",
         "116",
         "0",
         "0.7017545121546982",
         "0",
         "0"
        ],
        [
         "4",
         "3",
         "89",
         "1",
         "0.5836211903463085",
         "1",
         "1"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_links</th>\n",
       "      <th>num_words</th>\n",
       "      <th>has_offer</th>\n",
       "      <th>sender_score</th>\n",
       "      <th>all_caps</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.698901</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620466</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0.701755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.583621</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_links  num_words  has_offer  sender_score  all_caps  is_spam\n",
       "0          3         98          1      0.718607         0        0\n",
       "1          0        170          0      0.698901         1        0\n",
       "2          0         38          0      0.620466         0        0\n",
       "3          0        116          0      0.701755         0        0\n",
       "4          3         89          1      0.583621         1        1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c976756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: num_links       0\n",
      "num_words       0\n",
      "has_offer       0\n",
      "sender_score    0\n",
      "all_caps        0\n",
      "is_spam         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Missing values: {data.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e05477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatypes: num_links         int64\n",
      "num_words         int64\n",
      "has_offer         int64\n",
      "sender_score    float64\n",
      "all_caps          int64\n",
      "is_spam           int64\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Datatypes: {data.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "058f2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['num_links', 'num_words', 'has_offer', 'sender_score', 'all_caps']]\n",
    "y = data['is_spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c094531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled features sample:    num_links  num_words  has_offer  sender_score  all_caps\n",
      "0   1.229832  -0.224189          1      0.129767         0\n",
      "1  -1.227003   1.161143          0      0.025105         1\n",
      "2  -1.227003  -1.378632          0     -0.391486         0\n",
      "3  -1.227003   0.122144          0      0.040259         0\n",
      "4   1.229832  -0.397355          1     -0.587176         1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_features = ['num_links', 'num_words', 'sender_score']\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "print(f\"Scaled features sample: {X.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5157c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Non-Spam=17354, Spam=1746\n",
      "Imbalance ratio: 9.9:1\n",
      "\n",
      "Imbalance ratio: 9.9:1\n"
     ]
    }
   ],
   "source": [
    "# Quick class distribution check (no plot)\n",
    "spam_count = np.sum(data['is_spam'] == 1)\n",
    "non_spam_count = np.sum(data['is_spam'] == 0)\n",
    "print(f\"Class distribution: Non-Spam={non_spam_count}, Spam={spam_count}\")\n",
    "print(f\"Imbalance ratio: {non_spam_count/spam_count:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a804a",
   "metadata": {},
   "source": [
    "**We can notice class imbalance!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efcbf401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17354  1746] \n",
      "\n",
      "19100\n",
      "not-spam / spam: [0.55030541 5.4696449 ]\n"
     ]
    }
   ],
   "source": [
    "#Weight balancing using inverse frequency\n",
    "class_counts = np.bincount(y) #[17354, 1746]\n",
    "print(class_counts,\"\\n\")\n",
    "print(len(y))\n",
    "class_weights = len(y) / (class_counts * len(class_counts))\n",
    "\n",
    "print(f\"not-spam / spam: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1714a45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (15280, 5) (15280,)\n",
      "Validation set shape: (3820, 5) (3820,)\n",
      "Training class distribution: [0.9085733 0.0914267]\n",
      "Validation class distribution: [0.90863874 0.09136126]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.values, y, test_size=0.2, random_state=45, stratify=y\n",
    ")\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "print(\"\\nTraining set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Training class distribution:\", np.bincount(y_train) / len(y_train))\n",
    "print(\"Validation class distribution:\", np.bincount(y_val) / len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef009d",
   "metadata": {},
   "source": [
    "**let's begin with our logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a31725b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED LOGISTIC REGRESSION WITH DYNAMIC CLASS WEIGHTS\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    z = np.clip(z, -500, 500)  # Prevent overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_class_weights(y, weight_type='balanced'):\n",
    "    \"\"\"Compute dynamic class weights\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    class_counts = np.bincount(y)\n",
    "    \n",
    "    if weight_type == 'balanced':\n",
    "        # Standard balanced weights: n_samples / (n_classes * class_count)\n",
    "        weights = len(y) / (len(classes) * class_counts)\n",
    "    elif weight_type == 'inverse':\n",
    "        # Simple inverse frequency\n",
    "        weights = 1.0 / class_counts\n",
    "        weights = weights / np.sum(weights) * len(classes)  # Normalize\n",
    "    elif weight_type == 'log':\n",
    "        # Log-scaled weights for extreme imbalance\n",
    "        weights = np.log(len(y) / class_counts)\n",
    "    else:  # uniform\n",
    "        weights = np.ones(len(classes))\n",
    "    \n",
    "    # Create sample weights array\n",
    "    sample_weights = np.zeros(len(y))\n",
    "    for i, class_label in enumerate(classes):\n",
    "        sample_weights[y == class_label] = weights[i]\n",
    "    \n",
    "    return sample_weights, weights\n",
    "\n",
    "def compute_cost(X, y, w, b, lambda_l1=0.0, sample_weights=None):\n",
    "    \"\"\"Cost function with L1 regularization and class weights\"\"\"\n",
    "    m = X.shape[0]\n",
    "    z = np.dot(X, w) + b\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    # Prevent log(0) by clipping predictions\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Cross-entropy cost with sample weights\n",
    "    if sample_weights is not None:\n",
    "        cost = -np.mean(sample_weights * (y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)))\n",
    "    else:\n",
    "        cost = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    \n",
    "    # L1 regularization (Lasso)\n",
    "    l1_cost = lambda_l1 * np.sum(np.abs(w))\n",
    "    \n",
    "    return cost + l1_cost\n",
    "\n",
    "def compute_gradients(X, y, w, b, lambda_l1=0.0, sample_weights=None):\n",
    "    \"\"\"Compute gradients for gradient descent with class weights\"\"\"\n",
    "    m = X.shape[0]\n",
    "    z = np.dot(X, w) + b\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    # Gradients with sample weights\n",
    "    error = y_pred - y\n",
    "    if sample_weights is not None:\n",
    "        error = error * sample_weights\n",
    "    \n",
    "    dw = (1/m) * np.dot(X.T, error) + lambda_l1 * np.sign(w)\n",
    "    db = (1/m) * np.sum(error)\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "def gradient_descent(X, y, w, b, learning_rate=0.1, lambda_l1=0.0, num_iterations=1000, \n",
    "                    sample_weights=None):\n",
    "    \"\"\"Gradient descent optimization with class weights\"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Compute cost and gradients\n",
    "        cost = compute_cost(X, y, w, b, lambda_l1, sample_weights)\n",
    "        dw, db = compute_gradients(X, y, w, b, lambda_l1, sample_weights)\n",
    "        \n",
    "        # Update parameters\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Store cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return w, b, costs\n",
    "\n",
    "def predict(X, w, b, threshold=0.5):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    z = np.dot(X, w) + b\n",
    "    y_pred_proba = sigmoid(z)\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    return y_pred, y_pred_proba\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1,\n",
    "        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn\n",
    "    }\n",
    "\n",
    "def log_loss(y_true, y_pred):\n",
    "    eps  = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps,  1 - eps)\n",
    "    return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics\n",
    "def print_results(metrics, title=\"Model Results\"):\n",
    "    \"\"\"Print evaluation metrics in a clean format\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1_score']:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  TP: {metrics['tp']:4d} | FP: {metrics['fp']:4d}\")\n",
    "    print(f\"  FN: {metrics['fn']:4d} | TN: {metrics['tn']:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "babab7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRID SEARCH FOR OPTIMAL HYPERPARAMETERS\n",
      "==================================================\n",
      "Grid Search Parameters:\n",
      "  learning_rate: [0.01, 0.1, 0.5, 1.0]\n",
      "  lambda_l1: [0.0, 0.001, 0.01, 0.1]\n",
      "  num_iterations: [1000, 2000, 3000, 5000]\n",
      "  weight_type: ['uniform', 'balanced', 'inverse', 'log']\n",
      "\n",
      "Starting Grid Search...\n",
      "LR: 0.01 | L1: 0.0 | Iter: 1000 | Weight: uniform | Error: 0.3010 | F1: 0.0112\n",
      "LR: 0.01 | L1: 0.0 | Iter: 1000 | Weight: uniform | Error: 0.3010 | F1: 0.0112\n",
      "LR: 0.01 | L1: 0.0 | Iter: 1000 | Weight: balanced | Error: 0.4700 | F1: 0.3801\n",
      "LR: 0.01 | L1: 0.0 | Iter: 1000 | Weight: balanced | Error: 0.4700 | F1: 0.3801\n",
      "LR: 0.01 | L1: 0.0 | Iter: 1000 | Weight: inverse | Error: 0.1827 | F1: 0.3103\n",
      "LR: 0.01 | L1: 0.0 | Iter: 1000 | Weight: inverse | Error: 0.1827 | F1: 0.3103\n",
      "LR: 0.01 | L1: 0.0 | Iter: 1000 | Weight: log | Error: 0.1387 | F1: 0.2216\n",
      "LR: 0.01 | L1: 0.0 | Iter: 1000 | Weight: log | Error: 0.1387 | F1: 0.2216\n",
      "LR: 0.01 | L1: 0.0 | Iter: 2000 | Weight: uniform | Error: 0.2547 | F1: 0.0490\n",
      "LR: 0.01 | L1: 0.0 | Iter: 2000 | Weight: uniform | Error: 0.2547 | F1: 0.0490\n",
      "LR: 0.01 | L1: 0.0 | Iter: 2000 | Weight: balanced | Error: 0.4060 | F1: 0.4316\n",
      "LR: 0.01 | L1: 0.0 | Iter: 2000 | Weight: balanced | Error: 0.4060 | F1: 0.4316\n",
      "LR: 0.01 | L1: 0.0 | Iter: 2000 | Weight: inverse | Error: 0.1653 | F1: 0.3438\n",
      "LR: 0.01 | L1: 0.0 | Iter: 2000 | Weight: inverse | Error: 0.1653 | F1: 0.3438\n",
      "LR: 0.01 | L1: 0.0 | Iter: 2000 | Weight: log | Error: 0.1266 | F1: 0.2358\n",
      "LR: 0.01 | L1: 0.0 | Iter: 2000 | Weight: log | Error: 0.1266 | F1: 0.2358\n",
      "LR: 0.01 | L1: 0.0 | Iter: 3000 | Weight: uniform | Error: 0.2325 | F1: 0.1055\n",
      "LR: 0.01 | L1: 0.0 | Iter: 3000 | Weight: uniform | Error: 0.2325 | F1: 0.1055\n",
      "LR: 0.01 | L1: 0.0 | Iter: 3000 | Weight: balanced | Error: 0.3703 | F1: 0.4656\n",
      "LR: 0.01 | L1: 0.0 | Iter: 3000 | Weight: balanced | Error: 0.3703 | F1: 0.4656\n",
      "LR: 0.01 | L1: 0.0 | Iter: 3000 | Weight: inverse | Error: 0.1543 | F1: 0.3796\n",
      "LR: 0.01 | L1: 0.0 | Iter: 3000 | Weight: inverse | Error: 0.1543 | F1: 0.3796\n",
      "LR: 0.01 | L1: 0.0 | Iter: 3000 | Weight: log | Error: 0.1203 | F1: 0.2531\n",
      "LR: 0.01 | L1: 0.0 | Iter: 3000 | Weight: log | Error: 0.1203 | F1: 0.2531\n",
      "LR: 0.01 | L1: 0.0 | Iter: 5000 | Weight: uniform | Error: 0.2064 | F1: 0.2336\n",
      "LR: 0.01 | L1: 0.0 | Iter: 5000 | Weight: uniform | Error: 0.2064 | F1: 0.2336\n",
      "LR: 0.01 | L1: 0.0 | Iter: 5000 | Weight: balanced | Error: 0.3333 | F1: 0.4875\n",
      "LR: 0.01 | L1: 0.0 | Iter: 5000 | Weight: balanced | Error: 0.3333 | F1: 0.4875\n",
      "LR: 0.01 | L1: 0.0 | Iter: 5000 | Weight: inverse | Error: 0.1394 | F1: 0.4228\n",
      "LR: 0.01 | L1: 0.0 | Iter: 5000 | Weight: inverse | Error: 0.1394 | F1: 0.4228\n",
      "LR: 0.01 | L1: 0.0 | Iter: 5000 | Weight: log | Error: 0.1114 | F1: 0.2809\n",
      "LR: 0.01 | L1: 0.0 | Iter: 5000 | Weight: log | Error: 0.1114 | F1: 0.2809\n",
      "LR: 0.01 | L1: 0.001 | Iter: 1000 | Weight: uniform | Error: 0.3014 | F1: 0.0113\n",
      "LR: 0.01 | L1: 0.001 | Iter: 1000 | Weight: uniform | Error: 0.3014 | F1: 0.0113\n",
      "LR: 0.01 | L1: 0.001 | Iter: 1000 | Weight: balanced | Error: 0.4730 | F1: 0.3796\n",
      "LR: 0.01 | L1: 0.001 | Iter: 1000 | Weight: balanced | Error: 0.4730 | F1: 0.3796\n",
      "LR: 0.01 | L1: 0.001 | Iter: 1000 | Weight: inverse | Error: 0.1843 | F1: 0.3102\n",
      "LR: 0.01 | L1: 0.001 | Iter: 1000 | Weight: inverse | Error: 0.1843 | F1: 0.3102\n",
      "LR: 0.01 | L1: 0.001 | Iter: 1000 | Weight: log | Error: 0.1406 | F1: 0.2218\n",
      "LR: 0.01 | L1: 0.001 | Iter: 1000 | Weight: log | Error: 0.1406 | F1: 0.2218\n",
      "LR: 0.01 | L1: 0.001 | Iter: 2000 | Weight: uniform | Error: 0.2562 | F1: 0.0490\n",
      "LR: 0.01 | L1: 0.001 | Iter: 2000 | Weight: uniform | Error: 0.2562 | F1: 0.0490\n",
      "LR: 0.01 | L1: 0.001 | Iter: 2000 | Weight: balanced | Error: 0.4103 | F1: 0.4325\n",
      "LR: 0.01 | L1: 0.001 | Iter: 2000 | Weight: balanced | Error: 0.4103 | F1: 0.4325\n",
      "LR: 0.01 | L1: 0.001 | Iter: 2000 | Weight: inverse | Error: 0.1678 | F1: 0.3440\n",
      "LR: 0.01 | L1: 0.001 | Iter: 2000 | Weight: inverse | Error: 0.1678 | F1: 0.3440\n",
      "LR: 0.01 | L1: 0.001 | Iter: 2000 | Weight: log | Error: 0.1291 | F1: 0.2348\n",
      "LR: 0.01 | L1: 0.001 | Iter: 2000 | Weight: log | Error: 0.1291 | F1: 0.2348\n",
      "LR: 0.01 | L1: 0.001 | Iter: 3000 | Weight: uniform | Error: 0.2349 | F1: 0.0904\n",
      "LR: 0.01 | L1: 0.001 | Iter: 3000 | Weight: uniform | Error: 0.2349 | F1: 0.0904\n",
      "LR: 0.01 | L1: 0.001 | Iter: 3000 | Weight: balanced | Error: 0.3755 | F1: 0.4663\n",
      "LR: 0.01 | L1: 0.001 | Iter: 3000 | Weight: balanced | Error: 0.3755 | F1: 0.4663\n",
      "LR: 0.01 | L1: 0.001 | Iter: 3000 | Weight: inverse | Error: 0.1574 | F1: 0.3796\n",
      "LR: 0.01 | L1: 0.001 | Iter: 3000 | Weight: inverse | Error: 0.1574 | F1: 0.3796\n",
      "LR: 0.01 | L1: 0.001 | Iter: 3000 | Weight: log | Error: 0.1233 | F1: 0.2514\n",
      "LR: 0.01 | L1: 0.001 | Iter: 3000 | Weight: log | Error: 0.1233 | F1: 0.2514\n",
      "LR: 0.01 | L1: 0.001 | Iter: 5000 | Weight: uniform | Error: 0.2102 | F1: 0.2293\n",
      "LR: 0.01 | L1: 0.001 | Iter: 5000 | Weight: uniform | Error: 0.2102 | F1: 0.2293\n",
      "LR: 0.01 | L1: 0.001 | Iter: 5000 | Weight: balanced | Error: 0.3396 | F1: 0.4867\n",
      "LR: 0.01 | L1: 0.001 | Iter: 5000 | Weight: balanced | Error: 0.3396 | F1: 0.4867\n",
      "LR: 0.01 | L1: 0.001 | Iter: 5000 | Weight: inverse | Error: 0.1434 | F1: 0.4213\n",
      "LR: 0.01 | L1: 0.001 | Iter: 5000 | Weight: inverse | Error: 0.1434 | F1: 0.4213\n",
      "LR: 0.01 | L1: 0.001 | Iter: 5000 | Weight: log | Error: 0.1152 | F1: 0.2795\n",
      "LR: 0.01 | L1: 0.001 | Iter: 5000 | Weight: log | Error: 0.1152 | F1: 0.2795\n",
      "LR: 0.01 | L1: 0.01 | Iter: 1000 | Weight: uniform | Error: 0.3064 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.01 | Iter: 1000 | Weight: uniform | Error: 0.3064 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.01 | Iter: 1000 | Weight: balanced | Error: 0.4984 | F1: 0.3838\n",
      "LR: 0.01 | L1: 0.01 | Iter: 1000 | Weight: balanced | Error: 0.4984 | F1: 0.3838\n",
      "LR: 0.01 | L1: 0.01 | Iter: 1000 | Weight: inverse | Error: 0.1970 | F1: 0.3078\n",
      "LR: 0.01 | L1: 0.01 | Iter: 1000 | Weight: inverse | Error: 0.1970 | F1: 0.3078\n",
      "LR: 0.01 | L1: 0.01 | Iter: 1000 | Weight: log | Error: 0.1548 | F1: 0.2101\n",
      "LR: 0.01 | L1: 0.01 | Iter: 1000 | Weight: log | Error: 0.1548 | F1: 0.2101\n",
      "LR: 0.01 | L1: 0.01 | Iter: 2000 | Weight: uniform | Error: 0.2699 | F1: 0.0223\n",
      "LR: 0.01 | L1: 0.01 | Iter: 2000 | Weight: uniform | Error: 0.2699 | F1: 0.0223\n",
      "LR: 0.01 | L1: 0.01 | Iter: 2000 | Weight: balanced | Error: 0.4467 | F1: 0.4372\n",
      "LR: 0.01 | L1: 0.01 | Iter: 2000 | Weight: balanced | Error: 0.4467 | F1: 0.4372\n",
      "LR: 0.01 | L1: 0.01 | Iter: 2000 | Weight: inverse | Error: 0.1866 | F1: 0.3540\n",
      "LR: 0.01 | L1: 0.01 | Iter: 2000 | Weight: inverse | Error: 0.1866 | F1: 0.3540\n",
      "LR: 0.01 | L1: 0.01 | Iter: 2000 | Weight: log | Error: 0.1485 | F1: 0.2218\n",
      "LR: 0.01 | L1: 0.01 | Iter: 2000 | Weight: log | Error: 0.1485 | F1: 0.2218\n",
      "LR: 0.01 | L1: 0.01 | Iter: 3000 | Weight: uniform | Error: 0.2555 | F1: 0.0441\n",
      "LR: 0.01 | L1: 0.01 | Iter: 3000 | Weight: uniform | Error: 0.2555 | F1: 0.0441\n",
      "LR: 0.01 | L1: 0.01 | Iter: 3000 | Weight: balanced | Error: 0.4190 | F1: 0.4628\n",
      "LR: 0.01 | L1: 0.01 | Iter: 3000 | Weight: balanced | Error: 0.4190 | F1: 0.4628\n",
      "LR: 0.01 | L1: 0.01 | Iter: 3000 | Weight: inverse | Error: 0.1805 | F1: 0.3916\n",
      "LR: 0.01 | L1: 0.01 | Iter: 3000 | Weight: inverse | Error: 0.1805 | F1: 0.3916\n",
      "LR: 0.01 | L1: 0.01 | Iter: 3000 | Weight: log | Error: 0.1460 | F1: 0.2350\n",
      "LR: 0.01 | L1: 0.01 | Iter: 3000 | Weight: log | Error: 0.1460 | F1: 0.2350\n",
      "LR: 0.01 | L1: 0.01 | Iter: 5000 | Weight: uniform | Error: 0.2403 | F1: 0.0909\n",
      "LR: 0.01 | L1: 0.01 | Iter: 5000 | Weight: uniform | Error: 0.2403 | F1: 0.0909\n",
      "LR: 0.01 | L1: 0.01 | Iter: 5000 | Weight: balanced | Error: 0.3923 | F1: 0.4835\n",
      "LR: 0.01 | L1: 0.01 | Iter: 5000 | Weight: balanced | Error: 0.3923 | F1: 0.4835\n",
      "LR: 0.01 | L1: 0.01 | Iter: 5000 | Weight: inverse | Error: 0.1729 | F1: 0.4226\n",
      "LR: 0.01 | L1: 0.01 | Iter: 5000 | Weight: inverse | Error: 0.1729 | F1: 0.4226\n",
      "LR: 0.01 | L1: 0.01 | Iter: 5000 | Weight: log | Error: 0.1429 | F1: 0.2527\n",
      "LR: 0.01 | L1: 0.01 | Iter: 5000 | Weight: log | Error: 0.1429 | F1: 0.2527\n",
      "LR: 0.01 | L1: 0.1 | Iter: 1000 | Weight: uniform | Error: 0.3291 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.1 | Iter: 1000 | Weight: uniform | Error: 0.3291 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.1 | Iter: 1000 | Weight: balanced | Error: 0.6402 | F1: 0.4044\n",
      "LR: 0.01 | L1: 0.1 | Iter: 1000 | Weight: balanced | Error: 0.6402 | F1: 0.4044\n",
      "LR: 0.01 | L1: 0.1 | Iter: 1000 | Weight: inverse | Error: 0.2305 | F1: 0.2897\n",
      "LR: 0.01 | L1: 0.1 | Iter: 1000 | Weight: inverse | Error: 0.2305 | F1: 0.2897\n",
      "LR: 0.01 | L1: 0.1 | Iter: 1000 | Weight: log | Error: 0.1912 | F1: 0.1674\n",
      "LR: 0.01 | L1: 0.1 | Iter: 1000 | Weight: log | Error: 0.1912 | F1: 0.1674\n",
      "LR: 0.01 | L1: 0.1 | Iter: 2000 | Weight: uniform | Error: 0.3089 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.1 | Iter: 2000 | Weight: uniform | Error: 0.3089 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.1 | Iter: 2000 | Weight: balanced | Error: 0.6360 | F1: 0.4044\n",
      "LR: 0.01 | L1: 0.1 | Iter: 2000 | Weight: balanced | Error: 0.6360 | F1: 0.4044\n",
      "LR: 0.01 | L1: 0.1 | Iter: 2000 | Weight: inverse | Error: 0.2305 | F1: 0.3076\n",
      "LR: 0.01 | L1: 0.1 | Iter: 2000 | Weight: inverse | Error: 0.2305 | F1: 0.3076\n",
      "LR: 0.01 | L1: 0.1 | Iter: 2000 | Weight: log | Error: 0.1850 | F1: 0.1674\n",
      "LR: 0.01 | L1: 0.1 | Iter: 2000 | Weight: log | Error: 0.1850 | F1: 0.1674\n",
      "LR: 0.01 | L1: 0.1 | Iter: 3000 | Weight: uniform | Error: 0.3064 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.1 | Iter: 3000 | Weight: uniform | Error: 0.3064 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.1 | Iter: 3000 | Weight: balanced | Error: 0.6344 | F1: 0.4044\n",
      "LR: 0.01 | L1: 0.1 | Iter: 3000 | Weight: balanced | Error: 0.6344 | F1: 0.4044\n",
      "LR: 0.01 | L1: 0.1 | Iter: 3000 | Weight: inverse | Error: 0.2305 | F1: 0.3443\n",
      "LR: 0.01 | L1: 0.1 | Iter: 3000 | Weight: inverse | Error: 0.2305 | F1: 0.3443\n",
      "LR: 0.01 | L1: 0.1 | Iter: 3000 | Weight: log | Error: 0.1835 | F1: 0.1674\n",
      "LR: 0.01 | L1: 0.1 | Iter: 3000 | Weight: log | Error: 0.1835 | F1: 0.1674\n",
      "LR: 0.01 | L1: 0.1 | Iter: 5000 | Weight: uniform | Error: 0.3060 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.1 | Iter: 5000 | Weight: uniform | Error: 0.3060 | F1: 0.0000\n",
      "LR: 0.01 | L1: 0.1 | Iter: 5000 | Weight: balanced | Error: 0.6333 | F1: 0.4044\n",
      "LR: 0.01 | L1: 0.1 | Iter: 5000 | Weight: balanced | Error: 0.6333 | F1: 0.4044\n",
      "LR: 0.01 | L1: 0.1 | Iter: 5000 | Weight: inverse | Error: 0.2305 | F1: 0.2568\n",
      "LR: 0.01 | L1: 0.1 | Iter: 5000 | Weight: inverse | Error: 0.2305 | F1: 0.2568\n",
      "LR: 0.01 | L1: 0.1 | Iter: 5000 | Weight: log | Error: 0.1829 | F1: 0.1674\n",
      "LR: 0.01 | L1: 0.1 | Iter: 5000 | Weight: log | Error: 0.1829 | F1: 0.1674\n",
      "LR: 0.1 | L1: 0.0 | Iter: 1000 | Weight: uniform | Error: 0.1809 | F1: 0.4310\n",
      "LR: 0.1 | L1: 0.0 | Iter: 1000 | Weight: uniform | Error: 0.1809 | F1: 0.4310\n",
      "LR: 0.1 | L1: 0.0 | Iter: 1000 | Weight: balanced | Error: 0.3045 | F1: 0.4975\n",
      "LR: 0.1 | L1: 0.0 | Iter: 1000 | Weight: balanced | Error: 0.3045 | F1: 0.4975\n",
      "LR: 0.1 | L1: 0.0 | Iter: 1000 | Weight: inverse | Error: 0.1222 | F1: 0.4716\n",
      "LR: 0.1 | L1: 0.0 | Iter: 1000 | Weight: inverse | Error: 0.1222 | F1: 0.4716\n",
      "LR: 0.1 | L1: 0.0 | Iter: 1000 | Weight: log | Error: 0.0996 | F1: 0.3306\n",
      "LR: 0.1 | L1: 0.0 | Iter: 1000 | Weight: log | Error: 0.0996 | F1: 0.3306\n",
      "LR: 0.1 | L1: 0.0 | Iter: 2000 | Weight: uniform | Error: 0.1605 | F1: 0.5289\n",
      "LR: 0.1 | L1: 0.0 | Iter: 2000 | Weight: uniform | Error: 0.1605 | F1: 0.5289\n",
      "LR: 0.1 | L1: 0.0 | Iter: 2000 | Weight: balanced | Error: 0.2875 | F1: 0.5232\n",
      "LR: 0.1 | L1: 0.0 | Iter: 2000 | Weight: balanced | Error: 0.2875 | F1: 0.5232\n",
      "LR: 0.1 | L1: 0.0 | Iter: 2000 | Weight: inverse | Error: 0.1061 | F1: 0.4893\n",
      "LR: 0.1 | L1: 0.0 | Iter: 2000 | Weight: inverse | Error: 0.1061 | F1: 0.4893\n",
      "LR: 0.1 | L1: 0.0 | Iter: 2000 | Weight: log | Error: 0.0864 | F1: 0.3783\n",
      "LR: 0.1 | L1: 0.0 | Iter: 2000 | Weight: log | Error: 0.0864 | F1: 0.3783\n",
      "LR: 0.1 | L1: 0.0 | Iter: 3000 | Weight: uniform | Error: 0.1539 | F1: 0.5566\n",
      "LR: 0.1 | L1: 0.0 | Iter: 3000 | Weight: uniform | Error: 0.1539 | F1: 0.5566\n",
      "LR: 0.1 | L1: 0.0 | Iter: 3000 | Weight: balanced | Error: 0.2838 | F1: 0.5256\n",
      "LR: 0.1 | L1: 0.0 | Iter: 3000 | Weight: balanced | Error: 0.2838 | F1: 0.5256\n",
      "LR: 0.1 | L1: 0.0 | Iter: 3000 | Weight: inverse | Error: 0.1004 | F1: 0.4967\n",
      "LR: 0.1 | L1: 0.0 | Iter: 3000 | Weight: inverse | Error: 0.1004 | F1: 0.4967\n",
      "LR: 0.1 | L1: 0.0 | Iter: 3000 | Weight: log | Error: 0.0811 | F1: 0.3974\n",
      "LR: 0.1 | L1: 0.0 | Iter: 3000 | Weight: log | Error: 0.0811 | F1: 0.3974\n",
      "LR: 0.1 | L1: 0.0 | Iter: 5000 | Weight: uniform | Error: 0.1495 | F1: 0.5734\n",
      "LR: 0.1 | L1: 0.0 | Iter: 5000 | Weight: uniform | Error: 0.1495 | F1: 0.5734\n",
      "LR: 0.1 | L1: 0.0 | Iter: 5000 | Weight: balanced | Error: 0.2824 | F1: 0.5283\n",
      "LR: 0.1 | L1: 0.0 | Iter: 5000 | Weight: balanced | Error: 0.2824 | F1: 0.5283\n",
      "LR: 0.1 | L1: 0.0 | Iter: 5000 | Weight: inverse | Error: 0.0963 | F1: 0.5178\n",
      "LR: 0.1 | L1: 0.0 | Iter: 5000 | Weight: inverse | Error: 0.0963 | F1: 0.5178\n",
      "LR: 0.1 | L1: 0.0 | Iter: 5000 | Weight: log | Error: 0.0770 | F1: 0.4202\n",
      "LR: 0.1 | L1: 0.0 | Iter: 5000 | Weight: log | Error: 0.0770 | F1: 0.4202\n",
      "LR: 0.1 | L1: 0.001 | Iter: 1000 | Weight: uniform | Error: 0.1862 | F1: 0.4103\n",
      "LR: 0.1 | L1: 0.001 | Iter: 1000 | Weight: uniform | Error: 0.1862 | F1: 0.4103\n",
      "LR: 0.1 | L1: 0.001 | Iter: 1000 | Weight: balanced | Error: 0.3122 | F1: 0.4967\n",
      "LR: 0.1 | L1: 0.001 | Iter: 1000 | Weight: balanced | Error: 0.3122 | F1: 0.4967\n",
      "LR: 0.1 | L1: 0.001 | Iter: 1000 | Weight: inverse | Error: 0.1274 | F1: 0.4741\n",
      "LR: 0.1 | L1: 0.001 | Iter: 1000 | Weight: inverse | Error: 0.1274 | F1: 0.4741\n",
      "LR: 0.1 | L1: 0.001 | Iter: 1000 | Weight: log | Error: 0.1045 | F1: 0.3266\n",
      "LR: 0.1 | L1: 0.001 | Iter: 1000 | Weight: log | Error: 0.1045 | F1: 0.3266\n",
      "LR: 0.1 | L1: 0.001 | Iter: 2000 | Weight: uniform | Error: 0.1676 | F1: 0.5057\n",
      "LR: 0.1 | L1: 0.001 | Iter: 2000 | Weight: uniform | Error: 0.1676 | F1: 0.5057\n",
      "LR: 0.1 | L1: 0.001 | Iter: 2000 | Weight: balanced | Error: 0.2965 | F1: 0.5240\n",
      "LR: 0.1 | L1: 0.001 | Iter: 2000 | Weight: balanced | Error: 0.2965 | F1: 0.5240\n",
      "LR: 0.1 | L1: 0.001 | Iter: 2000 | Weight: inverse | Error: 0.1129 | F1: 0.4822\n",
      "LR: 0.1 | L1: 0.001 | Iter: 2000 | Weight: inverse | Error: 0.1129 | F1: 0.4822\n",
      "LR: 0.1 | L1: 0.001 | Iter: 2000 | Weight: log | Error: 0.0929 | F1: 0.3760\n",
      "LR: 0.1 | L1: 0.001 | Iter: 2000 | Weight: log | Error: 0.0929 | F1: 0.3760\n",
      "LR: 0.1 | L1: 0.001 | Iter: 3000 | Weight: uniform | Error: 0.1619 | F1: 0.5497\n",
      "LR: 0.1 | L1: 0.001 | Iter: 3000 | Weight: uniform | Error: 0.1619 | F1: 0.5497\n",
      "LR: 0.1 | L1: 0.001 | Iter: 3000 | Weight: balanced | Error: 0.2934 | F1: 0.5251\n",
      "LR: 0.1 | L1: 0.001 | Iter: 3000 | Weight: balanced | Error: 0.2934 | F1: 0.5251\n",
      "LR: 0.1 | L1: 0.001 | Iter: 3000 | Weight: inverse | Error: 0.1081 | F1: 0.4886\n",
      "LR: 0.1 | L1: 0.001 | Iter: 3000 | Weight: inverse | Error: 0.1081 | F1: 0.4886\n",
      "LR: 0.1 | L1: 0.001 | Iter: 3000 | Weight: log | Error: 0.0884 | F1: 0.3952\n",
      "LR: 0.1 | L1: 0.001 | Iter: 3000 | Weight: log | Error: 0.0884 | F1: 0.3952\n",
      "LR: 0.1 | L1: 0.001 | Iter: 5000 | Weight: uniform | Error: 0.1583 | F1: 0.5699\n",
      "LR: 0.1 | L1: 0.001 | Iter: 5000 | Weight: uniform | Error: 0.1583 | F1: 0.5699\n",
      "LR: 0.1 | L1: 0.001 | Iter: 5000 | Weight: balanced | Error: 0.2923 | F1: 0.5279\n",
      "LR: 0.1 | L1: 0.001 | Iter: 5000 | Weight: balanced | Error: 0.2923 | F1: 0.5279\n",
      "LR: 0.1 | L1: 0.001 | Iter: 5000 | Weight: inverse | Error: 0.1048 | F1: 0.5178\n",
      "LR: 0.1 | L1: 0.001 | Iter: 5000 | Weight: inverse | Error: 0.1048 | F1: 0.5178\n",
      "LR: 0.1 | L1: 0.001 | Iter: 5000 | Weight: log | Error: 0.0853 | F1: 0.4159\n",
      "LR: 0.1 | L1: 0.001 | Iter: 5000 | Weight: log | Error: 0.0853 | F1: 0.4159\n",
      "LR: 0.1 | L1: 0.01 | Iter: 1000 | Weight: uniform | Error: 0.2271 | F1: 0.2476\n",
      "LR: 0.1 | L1: 0.01 | Iter: 1000 | Weight: uniform | Error: 0.2271 | F1: 0.2476\n",
      "LR: 0.1 | L1: 0.01 | Iter: 1000 | Weight: balanced | Error: 0.3746 | F1: 0.4858\n",
      "LR: 0.1 | L1: 0.01 | Iter: 1000 | Weight: balanced | Error: 0.3746 | F1: 0.4858\n",
      "LR: 0.1 | L1: 0.01 | Iter: 1000 | Weight: inverse | Error: 0.1651 | F1: 0.4631\n",
      "LR: 0.1 | L1: 0.01 | Iter: 1000 | Weight: inverse | Error: 0.1651 | F1: 0.4631\n",
      "LR: 0.1 | L1: 0.01 | Iter: 1000 | Weight: log | Error: 0.1390 | F1: 0.2867\n",
      "LR: 0.1 | L1: 0.01 | Iter: 1000 | Weight: log | Error: 0.1390 | F1: 0.2867\n",
      "LR: 0.1 | L1: 0.01 | Iter: 2000 | Weight: uniform | Error: 0.2192 | F1: 0.3786\n",
      "LR: 0.1 | L1: 0.01 | Iter: 2000 | Weight: uniform | Error: 0.2192 | F1: 0.3786\n",
      "LR: 0.1 | L1: 0.01 | Iter: 2000 | Weight: balanced | Error: 0.3674 | F1: 0.4983\n",
      "LR: 0.1 | L1: 0.01 | Iter: 2000 | Weight: balanced | Error: 0.3674 | F1: 0.4983\n",
      "LR: 0.1 | L1: 0.01 | Iter: 2000 | Weight: inverse | Error: 0.1598 | F1: 0.4659\n",
      "LR: 0.1 | L1: 0.01 | Iter: 2000 | Weight: inverse | Error: 0.1598 | F1: 0.4659\n",
      "LR: 0.1 | L1: 0.01 | Iter: 2000 | Weight: log | Error: 0.1356 | F1: 0.3306\n",
      "LR: 0.1 | L1: 0.01 | Iter: 2000 | Weight: log | Error: 0.1356 | F1: 0.3306\n",
      "LR: 0.1 | L1: 0.01 | Iter: 3000 | Weight: uniform | Error: 0.2178 | F1: 0.4267\n",
      "LR: 0.1 | L1: 0.01 | Iter: 3000 | Weight: uniform | Error: 0.2178 | F1: 0.4267\n",
      "LR: 0.1 | L1: 0.01 | Iter: 3000 | Weight: balanced | Error: 0.3667 | F1: 0.5077\n",
      "LR: 0.1 | L1: 0.01 | Iter: 3000 | Weight: balanced | Error: 0.3667 | F1: 0.5077\n",
      "LR: 0.1 | L1: 0.01 | Iter: 3000 | Weight: inverse | Error: 0.1588 | F1: 0.4748\n",
      "LR: 0.1 | L1: 0.01 | Iter: 3000 | Weight: inverse | Error: 0.1588 | F1: 0.4748\n",
      "LR: 0.1 | L1: 0.01 | Iter: 3000 | Weight: log | Error: 0.1349 | F1: 0.3453\n",
      "LR: 0.1 | L1: 0.01 | Iter: 3000 | Weight: log | Error: 0.1349 | F1: 0.3453\n",
      "LR: 0.1 | L1: 0.01 | Iter: 5000 | Weight: uniform | Error: 0.2173 | F1: 0.4587\n",
      "LR: 0.1 | L1: 0.01 | Iter: 5000 | Weight: uniform | Error: 0.2173 | F1: 0.4587\n",
      "LR: 0.1 | L1: 0.01 | Iter: 5000 | Weight: balanced | Error: 0.3666 | F1: 0.5120\n",
      "LR: 0.1 | L1: 0.01 | Iter: 5000 | Weight: balanced | Error: 0.3666 | F1: 0.5120\n",
      "LR: 0.1 | L1: 0.01 | Iter: 5000 | Weight: inverse | Error: 0.1585 | F1: 0.4755\n",
      "LR: 0.1 | L1: 0.01 | Iter: 5000 | Weight: inverse | Error: 0.1585 | F1: 0.4755\n",
      "LR: 0.1 | L1: 0.01 | Iter: 5000 | Weight: log | Error: 0.1346 | F1: 0.3528\n",
      "LR: 0.1 | L1: 0.01 | Iter: 5000 | Weight: log | Error: 0.1346 | F1: 0.3528\n",
      "LR: 0.1 | L1: 0.1 | Iter: 1000 | Weight: uniform | Error: 0.3075 | F1: 0.0000\n",
      "LR: 0.1 | L1: 0.1 | Iter: 1000 | Weight: uniform | Error: 0.3075 | F1: 0.0000\n",
      "LR: 0.1 | L1: 0.1 | Iter: 1000 | Weight: balanced | Error: 0.6332 | F1: 0.4044\n",
      "LR: 0.1 | L1: 0.1 | Iter: 1000 | Weight: balanced | Error: 0.6332 | F1: 0.4044\n",
      "LR: 0.1 | L1: 0.1 | Iter: 1000 | Weight: inverse | Error: 0.2323 | F1: 0.4567\n",
      "LR: 0.1 | L1: 0.1 | Iter: 1000 | Weight: inverse | Error: 0.2323 | F1: 0.4567\n",
      "LR: 0.1 | L1: 0.1 | Iter: 1000 | Weight: log | Error: 0.1847 | F1: 0.1674\n",
      "LR: 0.1 | L1: 0.1 | Iter: 1000 | Weight: log | Error: 0.1847 | F1: 0.1674\n",
      "LR: 0.1 | L1: 0.1 | Iter: 2000 | Weight: uniform | Error: 0.3082 | F1: 0.0000\n",
      "LR: 0.1 | L1: 0.1 | Iter: 2000 | Weight: uniform | Error: 0.3082 | F1: 0.0000\n",
      "LR: 0.1 | L1: 0.1 | Iter: 2000 | Weight: balanced | Error: 0.6335 | F1: 0.4044\n",
      "LR: 0.1 | L1: 0.1 | Iter: 2000 | Weight: balanced | Error: 0.6335 | F1: 0.4044\n",
      "LR: 0.1 | L1: 0.1 | Iter: 2000 | Weight: inverse | Error: 0.2325 | F1: 0.2178\n",
      "LR: 0.1 | L1: 0.1 | Iter: 2000 | Weight: inverse | Error: 0.2325 | F1: 0.2178\n",
      "LR: 0.1 | L1: 0.1 | Iter: 2000 | Weight: log | Error: 0.1847 | F1: 0.1674\n",
      "LR: 0.1 | L1: 0.1 | Iter: 2000 | Weight: log | Error: 0.1847 | F1: 0.1674\n",
      "LR: 0.1 | L1: 0.1 | Iter: 3000 | Weight: uniform | Error: 0.3075 | F1: 0.0000\n",
      "LR: 0.1 | L1: 0.1 | Iter: 3000 | Weight: uniform | Error: 0.3075 | F1: 0.0000\n",
      "LR: 0.1 | L1: 0.1 | Iter: 3000 | Weight: balanced | Error: 0.6334 | F1: 0.4044\n",
      "LR: 0.1 | L1: 0.1 | Iter: 3000 | Weight: balanced | Error: 0.6334 | F1: 0.4044\n",
      "LR: 0.1 | L1: 0.1 | Iter: 3000 | Weight: inverse | Error: 0.2315 | F1: 0.2446\n",
      "LR: 0.1 | L1: 0.1 | Iter: 3000 | Weight: inverse | Error: 0.2315 | F1: 0.2446\n",
      "LR: 0.1 | L1: 0.1 | Iter: 3000 | Weight: log | Error: 0.1845 | F1: 0.1674\n",
      "LR: 0.1 | L1: 0.1 | Iter: 3000 | Weight: log | Error: 0.1845 | F1: 0.1674\n",
      "LR: 0.1 | L1: 0.1 | Iter: 5000 | Weight: uniform | Error: 0.3080 | F1: 0.0000\n",
      "LR: 0.1 | L1: 0.1 | Iter: 5000 | Weight: uniform | Error: 0.3080 | F1: 0.0000\n",
      "LR: 0.1 | L1: 0.1 | Iter: 5000 | Weight: balanced | Error: 0.6335 | F1: 0.4044\n",
      "LR: 0.1 | L1: 0.1 | Iter: 5000 | Weight: balanced | Error: 0.6335 | F1: 0.4044\n",
      "LR: 0.1 | L1: 0.1 | Iter: 5000 | Weight: inverse | Error: 0.2320 | F1: 0.3455\n",
      "LR: 0.1 | L1: 0.1 | Iter: 5000 | Weight: inverse | Error: 0.2320 | F1: 0.3455\n",
      "LR: 0.1 | L1: 0.1 | Iter: 5000 | Weight: log | Error: 0.1845 | F1: 0.1674\n",
      "LR: 0.1 | L1: 0.1 | Iter: 5000 | Weight: log | Error: 0.1845 | F1: 0.1674\n",
      "LR: 0.5 | L1: 0.0 | Iter: 1000 | Weight: uniform | Error: 0.1500 | F1: 0.5734\n",
      "LR: 0.5 | L1: 0.0 | Iter: 1000 | Weight: uniform | Error: 0.1500 | F1: 0.5734\n",
      "LR: 0.5 | L1: 0.0 | Iter: 1000 | Weight: balanced | Error: 0.2825 | F1: 0.5283\n",
      "LR: 0.5 | L1: 0.0 | Iter: 1000 | Weight: balanced | Error: 0.2825 | F1: 0.5283\n",
      "LR: 0.5 | L1: 0.0 | Iter: 1000 | Weight: inverse | Error: 0.0967 | F1: 0.5178\n",
      "LR: 0.5 | L1: 0.0 | Iter: 1000 | Weight: inverse | Error: 0.0967 | F1: 0.5178\n",
      "LR: 0.5 | L1: 0.0 | Iter: 1000 | Weight: log | Error: 0.0775 | F1: 0.4202\n",
      "LR: 0.5 | L1: 0.0 | Iter: 1000 | Weight: log | Error: 0.0775 | F1: 0.4202\n",
      "LR: 0.5 | L1: 0.0 | Iter: 2000 | Weight: uniform | Error: 0.1476 | F1: 0.5940\n",
      "LR: 0.5 | L1: 0.0 | Iter: 2000 | Weight: uniform | Error: 0.1476 | F1: 0.5940\n",
      "LR: 0.5 | L1: 0.0 | Iter: 2000 | Weight: balanced | Error: 0.2821 | F1: 0.5329\n",
      "LR: 0.5 | L1: 0.0 | Iter: 2000 | Weight: balanced | Error: 0.2821 | F1: 0.5329\n",
      "LR: 0.5 | L1: 0.0 | Iter: 2000 | Weight: inverse | Error: 0.0942 | F1: 0.5260\n",
      "LR: 0.5 | L1: 0.0 | Iter: 2000 | Weight: inverse | Error: 0.0942 | F1: 0.5260\n",
      "LR: 0.5 | L1: 0.0 | Iter: 2000 | Weight: log | Error: 0.0748 | F1: 0.4345\n",
      "LR: 0.5 | L1: 0.0 | Iter: 2000 | Weight: log | Error: 0.0748 | F1: 0.4345\n",
      "LR: 0.5 | L1: 0.0 | Iter: 3000 | Weight: uniform | Error: 0.1473 | F1: 0.6003\n",
      "LR: 0.5 | L1: 0.0 | Iter: 3000 | Weight: uniform | Error: 0.1473 | F1: 0.6003\n",
      "LR: 0.5 | L1: 0.0 | Iter: 3000 | Weight: balanced | Error: 0.2821 | F1: 0.5329\n",
      "LR: 0.5 | L1: 0.0 | Iter: 3000 | Weight: balanced | Error: 0.2821 | F1: 0.5329\n",
      "LR: 0.5 | L1: 0.0 | Iter: 3000 | Weight: inverse | Error: 0.0938 | F1: 0.5283\n",
      "LR: 0.5 | L1: 0.0 | Iter: 3000 | Weight: inverse | Error: 0.0938 | F1: 0.5283\n",
      "LR: 0.5 | L1: 0.0 | Iter: 3000 | Weight: log | Error: 0.0744 | F1: 0.4377\n",
      "LR: 0.5 | L1: 0.0 | Iter: 3000 | Weight: log | Error: 0.0744 | F1: 0.4377\n",
      "LR: 0.5 | L1: 0.0 | Iter: 5000 | Weight: uniform | Error: 0.1473 | F1: 0.6050\n",
      "LR: 0.5 | L1: 0.0 | Iter: 5000 | Weight: uniform | Error: 0.1473 | F1: 0.6050\n",
      "LR: 0.5 | L1: 0.0 | Iter: 5000 | Weight: balanced | Error: 0.2821 | F1: 0.5333\n",
      "LR: 0.5 | L1: 0.0 | Iter: 5000 | Weight: balanced | Error: 0.2821 | F1: 0.5333\n",
      "LR: 0.5 | L1: 0.0 | Iter: 5000 | Weight: inverse | Error: 0.0938 | F1: 0.5319\n",
      "LR: 0.5 | L1: 0.0 | Iter: 5000 | Weight: inverse | Error: 0.0938 | F1: 0.5319\n",
      "LR: 0.5 | L1: 0.0 | Iter: 5000 | Weight: log | Error: 0.0743 | F1: 0.4398\n",
      "LR: 0.5 | L1: 0.0 | Iter: 5000 | Weight: log | Error: 0.0743 | F1: 0.4398\n",
      "LR: 0.5 | L1: 0.001 | Iter: 1000 | Weight: uniform | Error: 0.1587 | F1: 0.5699\n",
      "LR: 0.5 | L1: 0.001 | Iter: 1000 | Weight: uniform | Error: 0.1587 | F1: 0.5699\n",
      "LR: 0.5 | L1: 0.001 | Iter: 1000 | Weight: balanced | Error: 0.2923 | F1: 0.5279\n",
      "LR: 0.5 | L1: 0.001 | Iter: 1000 | Weight: balanced | Error: 0.2923 | F1: 0.5279\n",
      "LR: 0.5 | L1: 0.001 | Iter: 1000 | Weight: inverse | Error: 0.1052 | F1: 0.5178\n",
      "LR: 0.5 | L1: 0.001 | Iter: 1000 | Weight: inverse | Error: 0.1052 | F1: 0.5178\n",
      "LR: 0.5 | L1: 0.001 | Iter: 1000 | Weight: log | Error: 0.0856 | F1: 0.4159\n",
      "LR: 0.5 | L1: 0.001 | Iter: 1000 | Weight: log | Error: 0.0856 | F1: 0.4159\n",
      "LR: 0.5 | L1: 0.001 | Iter: 2000 | Weight: uniform | Error: 0.1570 | F1: 0.5874\n",
      "LR: 0.5 | L1: 0.001 | Iter: 2000 | Weight: uniform | Error: 0.1570 | F1: 0.5874\n",
      "LR: 0.5 | L1: 0.001 | Iter: 2000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 0.5 | L1: 0.001 | Iter: 2000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 0.5 | L1: 0.001 | Iter: 2000 | Weight: inverse | Error: 0.1035 | F1: 0.5233\n",
      "LR: 0.5 | L1: 0.001 | Iter: 2000 | Weight: inverse | Error: 0.1035 | F1: 0.5233\n",
      "LR: 0.5 | L1: 0.001 | Iter: 2000 | Weight: log | Error: 0.0839 | F1: 0.4293\n",
      "LR: 0.5 | L1: 0.001 | Iter: 2000 | Weight: log | Error: 0.0839 | F1: 0.4293\n",
      "LR: 0.5 | L1: 0.001 | Iter: 3000 | Weight: uniform | Error: 0.1568 | F1: 0.5878\n",
      "LR: 0.5 | L1: 0.001 | Iter: 3000 | Weight: uniform | Error: 0.1568 | F1: 0.5878\n",
      "LR: 0.5 | L1: 0.001 | Iter: 3000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 0.5 | L1: 0.001 | Iter: 3000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 0.5 | L1: 0.001 | Iter: 3000 | Weight: inverse | Error: 0.1033 | F1: 0.5256\n",
      "LR: 0.5 | L1: 0.001 | Iter: 3000 | Weight: inverse | Error: 0.1033 | F1: 0.5256\n",
      "LR: 0.5 | L1: 0.001 | Iter: 3000 | Weight: log | Error: 0.0837 | F1: 0.4321\n",
      "LR: 0.5 | L1: 0.001 | Iter: 3000 | Weight: log | Error: 0.0837 | F1: 0.4321\n",
      "LR: 0.5 | L1: 0.001 | Iter: 5000 | Weight: uniform | Error: 0.1568 | F1: 0.5892\n",
      "LR: 0.5 | L1: 0.001 | Iter: 5000 | Weight: uniform | Error: 0.1568 | F1: 0.5892\n",
      "LR: 0.5 | L1: 0.001 | Iter: 5000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 0.5 | L1: 0.001 | Iter: 5000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 0.5 | L1: 0.001 | Iter: 5000 | Weight: inverse | Error: 0.1033 | F1: 0.5274\n",
      "LR: 0.5 | L1: 0.001 | Iter: 5000 | Weight: inverse | Error: 0.1033 | F1: 0.5274\n",
      "LR: 0.5 | L1: 0.001 | Iter: 5000 | Weight: log | Error: 0.0836 | F1: 0.4344\n",
      "LR: 0.5 | L1: 0.001 | Iter: 5000 | Weight: log | Error: 0.0836 | F1: 0.4344\n",
      "LR: 0.5 | L1: 0.01 | Iter: 1000 | Weight: uniform | Error: 0.2173 | F1: 0.4587\n",
      "LR: 0.5 | L1: 0.01 | Iter: 1000 | Weight: uniform | Error: 0.2173 | F1: 0.4587\n",
      "LR: 0.5 | L1: 0.01 | Iter: 1000 | Weight: balanced | Error: 0.3666 | F1: 0.5120\n",
      "LR: 0.5 | L1: 0.01 | Iter: 1000 | Weight: balanced | Error: 0.3666 | F1: 0.5120\n",
      "LR: 0.5 | L1: 0.01 | Iter: 1000 | Weight: inverse | Error: 0.1585 | F1: 0.4747\n",
      "LR: 0.5 | L1: 0.01 | Iter: 1000 | Weight: inverse | Error: 0.1585 | F1: 0.4747\n",
      "LR: 0.5 | L1: 0.01 | Iter: 1000 | Weight: log | Error: 0.1346 | F1: 0.3526\n",
      "LR: 0.5 | L1: 0.01 | Iter: 1000 | Weight: log | Error: 0.1346 | F1: 0.3526\n",
      "LR: 0.5 | L1: 0.01 | Iter: 2000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 0.5 | L1: 0.01 | Iter: 2000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 0.5 | L1: 0.01 | Iter: 2000 | Weight: balanced | Error: 0.3666 | F1: 0.5129\n",
      "LR: 0.5 | L1: 0.01 | Iter: 2000 | Weight: balanced | Error: 0.3666 | F1: 0.5129\n",
      "LR: 0.5 | L1: 0.01 | Iter: 2000 | Weight: inverse | Error: 0.1585 | F1: 0.4799\n",
      "LR: 0.5 | L1: 0.01 | Iter: 2000 | Weight: inverse | Error: 0.1585 | F1: 0.4799\n",
      "LR: 0.5 | L1: 0.01 | Iter: 2000 | Weight: log | Error: 0.1346 | F1: 0.3578\n",
      "LR: 0.5 | L1: 0.01 | Iter: 2000 | Weight: log | Error: 0.1346 | F1: 0.3578\n",
      "LR: 0.5 | L1: 0.01 | Iter: 3000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 0.5 | L1: 0.01 | Iter: 3000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 0.5 | L1: 0.01 | Iter: 3000 | Weight: balanced | Error: 0.3666 | F1: 0.5116\n",
      "LR: 0.5 | L1: 0.01 | Iter: 3000 | Weight: balanced | Error: 0.3666 | F1: 0.5116\n",
      "LR: 0.5 | L1: 0.01 | Iter: 3000 | Weight: inverse | Error: 0.1585 | F1: 0.4795\n",
      "LR: 0.5 | L1: 0.01 | Iter: 3000 | Weight: inverse | Error: 0.1585 | F1: 0.4795\n",
      "LR: 0.5 | L1: 0.01 | Iter: 3000 | Weight: log | Error: 0.1346 | F1: 0.3578\n",
      "LR: 0.5 | L1: 0.01 | Iter: 3000 | Weight: log | Error: 0.1346 | F1: 0.3578\n",
      "LR: 0.5 | L1: 0.01 | Iter: 5000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 0.5 | L1: 0.01 | Iter: 5000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 0.5 | L1: 0.01 | Iter: 5000 | Weight: balanced | Error: 0.3666 | F1: 0.5129\n",
      "LR: 0.5 | L1: 0.01 | Iter: 5000 | Weight: balanced | Error: 0.3666 | F1: 0.5129\n",
      "LR: 0.5 | L1: 0.01 | Iter: 5000 | Weight: inverse | Error: 0.1585 | F1: 0.4795\n",
      "LR: 0.5 | L1: 0.01 | Iter: 5000 | Weight: inverse | Error: 0.1585 | F1: 0.4795\n",
      "LR: 0.5 | L1: 0.01 | Iter: 5000 | Weight: log | Error: 0.1346 | F1: 0.3578\n",
      "LR: 0.5 | L1: 0.01 | Iter: 5000 | Weight: log | Error: 0.1346 | F1: 0.3578\n",
      "LR: 0.5 | L1: 0.1 | Iter: 1000 | Weight: uniform | Error: 0.3147 | F1: 0.0000\n",
      "LR: 0.5 | L1: 0.1 | Iter: 1000 | Weight: uniform | Error: 0.3147 | F1: 0.0000\n",
      "LR: 0.5 | L1: 0.1 | Iter: 1000 | Weight: balanced | Error: 0.6392 | F1: 0.4155\n",
      "LR: 0.5 | L1: 0.1 | Iter: 1000 | Weight: balanced | Error: 0.6392 | F1: 0.4155\n",
      "LR: 0.5 | L1: 0.1 | Iter: 1000 | Weight: inverse | Error: 0.2381 | F1: 0.3214\n",
      "LR: 0.5 | L1: 0.1 | Iter: 1000 | Weight: inverse | Error: 0.2381 | F1: 0.3214\n",
      "LR: 0.5 | L1: 0.1 | Iter: 1000 | Weight: log | Error: 0.1911 | F1: 0.1674\n",
      "LR: 0.5 | L1: 0.1 | Iter: 1000 | Weight: log | Error: 0.1911 | F1: 0.1674\n",
      "LR: 0.5 | L1: 0.1 | Iter: 2000 | Weight: uniform | Error: 0.3110 | F1: 0.0000\n",
      "LR: 0.5 | L1: 0.1 | Iter: 2000 | Weight: uniform | Error: 0.3110 | F1: 0.0000\n",
      "LR: 0.5 | L1: 0.1 | Iter: 2000 | Weight: balanced | Error: 0.6378 | F1: 0.4261\n",
      "LR: 0.5 | L1: 0.1 | Iter: 2000 | Weight: balanced | Error: 0.6378 | F1: 0.4261\n",
      "LR: 0.5 | L1: 0.1 | Iter: 2000 | Weight: inverse | Error: 0.2355 | F1: 0.2933\n",
      "LR: 0.5 | L1: 0.1 | Iter: 2000 | Weight: inverse | Error: 0.2355 | F1: 0.2933\n",
      "LR: 0.5 | L1: 0.1 | Iter: 2000 | Weight: log | Error: 0.1935 | F1: 0.1674\n",
      "LR: 0.5 | L1: 0.1 | Iter: 2000 | Weight: log | Error: 0.1935 | F1: 0.1674\n",
      "LR: 0.5 | L1: 0.1 | Iter: 3000 | Weight: uniform | Error: 0.3152 | F1: 0.0000\n",
      "LR: 0.5 | L1: 0.1 | Iter: 3000 | Weight: uniform | Error: 0.3152 | F1: 0.0000\n",
      "LR: 0.5 | L1: 0.1 | Iter: 3000 | Weight: balanced | Error: 0.6378 | F1: 0.4044\n",
      "LR: 0.5 | L1: 0.1 | Iter: 3000 | Weight: balanced | Error: 0.6378 | F1: 0.4044\n",
      "LR: 0.5 | L1: 0.1 | Iter: 3000 | Weight: inverse | Error: 0.2379 | F1: 0.3578\n",
      "LR: 0.5 | L1: 0.1 | Iter: 3000 | Weight: inverse | Error: 0.2379 | F1: 0.3578\n",
      "LR: 0.5 | L1: 0.1 | Iter: 3000 | Weight: log | Error: 0.1932 | F1: 0.1674\n",
      "LR: 0.5 | L1: 0.1 | Iter: 3000 | Weight: log | Error: 0.1932 | F1: 0.1674\n",
      "LR: 0.5 | L1: 0.1 | Iter: 5000 | Weight: uniform | Error: 0.3153 | F1: 0.0000\n",
      "LR: 0.5 | L1: 0.1 | Iter: 5000 | Weight: uniform | Error: 0.3153 | F1: 0.0000\n",
      "LR: 0.5 | L1: 0.1 | Iter: 5000 | Weight: balanced | Error: 0.6383 | F1: 0.4044\n",
      "LR: 0.5 | L1: 0.1 | Iter: 5000 | Weight: balanced | Error: 0.6383 | F1: 0.4044\n",
      "LR: 0.5 | L1: 0.1 | Iter: 5000 | Weight: inverse | Error: 0.2393 | F1: 0.3193\n",
      "LR: 0.5 | L1: 0.1 | Iter: 5000 | Weight: inverse | Error: 0.2393 | F1: 0.3193\n",
      "LR: 0.5 | L1: 0.1 | Iter: 5000 | Weight: log | Error: 0.1952 | F1: 0.1674\n",
      "LR: 0.5 | L1: 0.1 | Iter: 5000 | Weight: log | Error: 0.1952 | F1: 0.1674\n",
      "LR: 1.0 | L1: 0.0 | Iter: 1000 | Weight: uniform | Error: 0.1477 | F1: 0.5940\n",
      "LR: 1.0 | L1: 0.0 | Iter: 1000 | Weight: uniform | Error: 0.1477 | F1: 0.5940\n",
      "LR: 1.0 | L1: 0.0 | Iter: 1000 | Weight: balanced | Error: 0.2821 | F1: 0.5329\n",
      "LR: 1.0 | L1: 0.0 | Iter: 1000 | Weight: balanced | Error: 0.2821 | F1: 0.5329\n",
      "LR: 1.0 | L1: 0.0 | Iter: 1000 | Weight: inverse | Error: 0.0943 | F1: 0.5260\n",
      "LR: 1.0 | L1: 0.0 | Iter: 1000 | Weight: inverse | Error: 0.0943 | F1: 0.5260\n",
      "LR: 1.0 | L1: 0.0 | Iter: 1000 | Weight: log | Error: 0.0749 | F1: 0.4345\n",
      "LR: 1.0 | L1: 0.0 | Iter: 1000 | Weight: log | Error: 0.0749 | F1: 0.4345\n",
      "LR: 1.0 | L1: 0.0 | Iter: 2000 | Weight: uniform | Error: 0.1473 | F1: 0.6050\n",
      "LR: 1.0 | L1: 0.0 | Iter: 2000 | Weight: uniform | Error: 0.1473 | F1: 0.6050\n",
      "LR: 1.0 | L1: 0.0 | Iter: 2000 | Weight: balanced | Error: 0.2821 | F1: 0.5333\n",
      "LR: 1.0 | L1: 0.0 | Iter: 2000 | Weight: balanced | Error: 0.2821 | F1: 0.5333\n",
      "LR: 1.0 | L1: 0.0 | Iter: 2000 | Weight: inverse | Error: 0.0938 | F1: 0.5305\n",
      "LR: 1.0 | L1: 0.0 | Iter: 2000 | Weight: inverse | Error: 0.0938 | F1: 0.5305\n",
      "LR: 1.0 | L1: 0.0 | Iter: 2000 | Weight: log | Error: 0.0743 | F1: 0.4392\n",
      "LR: 1.0 | L1: 0.0 | Iter: 2000 | Weight: log | Error: 0.0743 | F1: 0.4392\n",
      "LR: 1.0 | L1: 0.0 | Iter: 3000 | Weight: uniform | Error: 0.1473 | F1: 0.6050\n",
      "LR: 1.0 | L1: 0.0 | Iter: 3000 | Weight: uniform | Error: 0.1473 | F1: 0.6050\n",
      "LR: 1.0 | L1: 0.0 | Iter: 3000 | Weight: balanced | Error: 0.2821 | F1: 0.5333\n",
      "LR: 1.0 | L1: 0.0 | Iter: 3000 | Weight: balanced | Error: 0.2821 | F1: 0.5333\n",
      "LR: 1.0 | L1: 0.0 | Iter: 3000 | Weight: inverse | Error: 0.0937 | F1: 0.5329\n",
      "LR: 1.0 | L1: 0.0 | Iter: 3000 | Weight: inverse | Error: 0.0937 | F1: 0.5329\n",
      "LR: 1.0 | L1: 0.0 | Iter: 3000 | Weight: log | Error: 0.0743 | F1: 0.4404\n",
      "LR: 1.0 | L1: 0.0 | Iter: 3000 | Weight: log | Error: 0.0743 | F1: 0.4404\n",
      "LR: 1.0 | L1: 0.0 | Iter: 5000 | Weight: uniform | Error: 0.1473 | F1: 0.6050\n",
      "LR: 1.0 | L1: 0.0 | Iter: 5000 | Weight: uniform | Error: 0.1473 | F1: 0.6050\n",
      "LR: 1.0 | L1: 0.0 | Iter: 5000 | Weight: balanced | Error: 0.2821 | F1: 0.5333\n",
      "LR: 1.0 | L1: 0.0 | Iter: 5000 | Weight: balanced | Error: 0.2821 | F1: 0.5333\n",
      "LR: 1.0 | L1: 0.0 | Iter: 5000 | Weight: inverse | Error: 0.0937 | F1: 0.5333\n",
      "LR: 1.0 | L1: 0.0 | Iter: 5000 | Weight: inverse | Error: 0.0937 | F1: 0.5333\n",
      "LR: 1.0 | L1: 0.0 | Iter: 5000 | Weight: log | Error: 0.0743 | F1: 0.4413\n",
      "LR: 1.0 | L1: 0.0 | Iter: 5000 | Weight: log | Error: 0.0743 | F1: 0.4413\n",
      "LR: 1.0 | L1: 0.001 | Iter: 1000 | Weight: uniform | Error: 0.1570 | F1: 0.5874\n",
      "LR: 1.0 | L1: 0.001 | Iter: 1000 | Weight: uniform | Error: 0.1570 | F1: 0.5874\n",
      "LR: 1.0 | L1: 0.001 | Iter: 1000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 1.0 | L1: 0.001 | Iter: 1000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 1.0 | L1: 0.001 | Iter: 1000 | Weight: inverse | Error: 0.1035 | F1: 0.5233\n",
      "LR: 1.0 | L1: 0.001 | Iter: 1000 | Weight: inverse | Error: 0.1035 | F1: 0.5233\n",
      "LR: 1.0 | L1: 0.001 | Iter: 1000 | Weight: log | Error: 0.0839 | F1: 0.4293\n",
      "LR: 1.0 | L1: 0.001 | Iter: 1000 | Weight: log | Error: 0.0839 | F1: 0.4293\n",
      "LR: 1.0 | L1: 0.001 | Iter: 2000 | Weight: uniform | Error: 0.1568 | F1: 0.5892\n",
      "LR: 1.0 | L1: 0.001 | Iter: 2000 | Weight: uniform | Error: 0.1568 | F1: 0.5892\n",
      "LR: 1.0 | L1: 0.001 | Iter: 2000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 1.0 | L1: 0.001 | Iter: 2000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 1.0 | L1: 0.001 | Iter: 2000 | Weight: inverse | Error: 0.1033 | F1: 0.5270\n",
      "LR: 1.0 | L1: 0.001 | Iter: 2000 | Weight: inverse | Error: 0.1033 | F1: 0.5270\n",
      "LR: 1.0 | L1: 0.001 | Iter: 2000 | Weight: log | Error: 0.0837 | F1: 0.4341\n",
      "LR: 1.0 | L1: 0.001 | Iter: 2000 | Weight: log | Error: 0.0837 | F1: 0.4341\n",
      "LR: 1.0 | L1: 0.001 | Iter: 3000 | Weight: uniform | Error: 0.1568 | F1: 0.5892\n",
      "LR: 1.0 | L1: 0.001 | Iter: 3000 | Weight: uniform | Error: 0.1568 | F1: 0.5892\n",
      "LR: 1.0 | L1: 0.001 | Iter: 3000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 1.0 | L1: 0.001 | Iter: 3000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 1.0 | L1: 0.001 | Iter: 3000 | Weight: inverse | Error: 0.1033 | F1: 0.5274\n",
      "LR: 1.0 | L1: 0.001 | Iter: 3000 | Weight: inverse | Error: 0.1033 | F1: 0.5274\n",
      "LR: 1.0 | L1: 0.001 | Iter: 3000 | Weight: log | Error: 0.0836 | F1: 0.4344\n",
      "LR: 1.0 | L1: 0.001 | Iter: 3000 | Weight: log | Error: 0.0836 | F1: 0.4344\n",
      "LR: 1.0 | L1: 0.001 | Iter: 5000 | Weight: uniform | Error: 0.1568 | F1: 0.5892\n",
      "LR: 1.0 | L1: 0.001 | Iter: 5000 | Weight: uniform | Error: 0.1568 | F1: 0.5892\n",
      "LR: 1.0 | L1: 0.001 | Iter: 5000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 1.0 | L1: 0.001 | Iter: 5000 | Weight: balanced | Error: 0.2921 | F1: 0.5302\n",
      "LR: 1.0 | L1: 0.001 | Iter: 5000 | Weight: inverse | Error: 0.1033 | F1: 0.5274\n",
      "LR: 1.0 | L1: 0.001 | Iter: 5000 | Weight: inverse | Error: 0.1033 | F1: 0.5274\n",
      "LR: 1.0 | L1: 0.001 | Iter: 5000 | Weight: log | Error: 0.0836 | F1: 0.4344\n",
      "LR: 1.0 | L1: 0.001 | Iter: 5000 | Weight: log | Error: 0.0836 | F1: 0.4344\n",
      "LR: 1.0 | L1: 0.01 | Iter: 1000 | Weight: uniform | Error: 0.2173 | F1: 0.4549\n",
      "LR: 1.0 | L1: 0.01 | Iter: 1000 | Weight: uniform | Error: 0.2173 | F1: 0.4549\n",
      "LR: 1.0 | L1: 0.01 | Iter: 1000 | Weight: balanced | Error: 0.3666 | F1: 0.5120\n",
      "LR: 1.0 | L1: 0.01 | Iter: 1000 | Weight: balanced | Error: 0.3666 | F1: 0.5120\n",
      "LR: 1.0 | L1: 0.01 | Iter: 1000 | Weight: inverse | Error: 0.1586 | F1: 0.4799\n",
      "LR: 1.0 | L1: 0.01 | Iter: 1000 | Weight: inverse | Error: 0.1586 | F1: 0.4799\n",
      "LR: 1.0 | L1: 0.01 | Iter: 1000 | Weight: log | Error: 0.1346 | F1: 0.3580\n",
      "LR: 1.0 | L1: 0.01 | Iter: 1000 | Weight: log | Error: 0.1346 | F1: 0.3580\n",
      "LR: 1.0 | L1: 0.01 | Iter: 2000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 1.0 | L1: 0.01 | Iter: 2000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 1.0 | L1: 0.01 | Iter: 2000 | Weight: balanced | Error: 0.3666 | F1: 0.5129\n",
      "LR: 1.0 | L1: 0.01 | Iter: 2000 | Weight: balanced | Error: 0.3666 | F1: 0.5129\n",
      "LR: 1.0 | L1: 0.01 | Iter: 2000 | Weight: inverse | Error: 0.1586 | F1: 0.4795\n",
      "LR: 1.0 | L1: 0.01 | Iter: 2000 | Weight: inverse | Error: 0.1586 | F1: 0.4795\n",
      "LR: 1.0 | L1: 0.01 | Iter: 2000 | Weight: log | Error: 0.1346 | F1: 0.3580\n",
      "LR: 1.0 | L1: 0.01 | Iter: 2000 | Weight: log | Error: 0.1346 | F1: 0.3580\n",
      "LR: 1.0 | L1: 0.01 | Iter: 3000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 1.0 | L1: 0.01 | Iter: 3000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 1.0 | L1: 0.01 | Iter: 3000 | Weight: balanced | Error: 0.3666 | F1: 0.5120\n",
      "LR: 1.0 | L1: 0.01 | Iter: 3000 | Weight: balanced | Error: 0.3666 | F1: 0.5120\n",
      "LR: 1.0 | L1: 0.01 | Iter: 3000 | Weight: inverse | Error: 0.1586 | F1: 0.4795\n",
      "LR: 1.0 | L1: 0.01 | Iter: 3000 | Weight: inverse | Error: 0.1586 | F1: 0.4795\n",
      "LR: 1.0 | L1: 0.01 | Iter: 3000 | Weight: log | Error: 0.1346 | F1: 0.3580\n",
      "LR: 1.0 | L1: 0.01 | Iter: 3000 | Weight: log | Error: 0.1346 | F1: 0.3580\n",
      "LR: 1.0 | L1: 0.01 | Iter: 5000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 1.0 | L1: 0.01 | Iter: 5000 | Weight: uniform | Error: 0.2173 | F1: 0.4540\n",
      "LR: 1.0 | L1: 0.01 | Iter: 5000 | Weight: balanced | Error: 0.3666 | F1: 0.5129\n",
      "LR: 1.0 | L1: 0.01 | Iter: 5000 | Weight: balanced | Error: 0.3666 | F1: 0.5129\n",
      "LR: 1.0 | L1: 0.01 | Iter: 5000 | Weight: inverse | Error: 0.1586 | F1: 0.4795\n",
      "LR: 1.0 | L1: 0.01 | Iter: 5000 | Weight: inverse | Error: 0.1586 | F1: 0.4795\n",
      "LR: 1.0 | L1: 0.01 | Iter: 5000 | Weight: log | Error: 0.1346 | F1: 0.3580\n",
      "LR: 1.0 | L1: 0.01 | Iter: 5000 | Weight: log | Error: 0.1346 | F1: 0.3580\n",
      "LR: 1.0 | L1: 0.1 | Iter: 1000 | Weight: uniform | Error: 0.3231 | F1: 0.0000\n",
      "LR: 1.0 | L1: 0.1 | Iter: 1000 | Weight: uniform | Error: 0.3231 | F1: 0.0000\n",
      "LR: 1.0 | L1: 0.1 | Iter: 1000 | Weight: balanced | Error: 0.6423 | F1: 0.4275\n",
      "LR: 1.0 | L1: 0.1 | Iter: 1000 | Weight: balanced | Error: 0.6423 | F1: 0.4275\n",
      "LR: 1.0 | L1: 0.1 | Iter: 1000 | Weight: inverse | Error: 0.2528 | F1: 0.2929\n",
      "LR: 1.0 | L1: 0.1 | Iter: 1000 | Weight: inverse | Error: 0.2528 | F1: 0.2929\n",
      "LR: 1.0 | L1: 0.1 | Iter: 1000 | Weight: log | Error: 0.2029 | F1: 0.1674\n",
      "LR: 1.0 | L1: 0.1 | Iter: 1000 | Weight: log | Error: 0.2029 | F1: 0.1674\n",
      "LR: 1.0 | L1: 0.1 | Iter: 2000 | Weight: uniform | Error: 0.3274 | F1: 0.0000\n",
      "LR: 1.0 | L1: 0.1 | Iter: 2000 | Weight: uniform | Error: 0.3274 | F1: 0.0000\n",
      "LR: 1.0 | L1: 0.1 | Iter: 2000 | Weight: balanced | Error: 0.6486 | F1: 0.4261\n",
      "LR: 1.0 | L1: 0.1 | Iter: 2000 | Weight: balanced | Error: 0.6486 | F1: 0.4261\n",
      "LR: 1.0 | L1: 0.1 | Iter: 2000 | Weight: inverse | Error: 0.2501 | F1: 0.1486\n",
      "LR: 1.0 | L1: 0.1 | Iter: 2000 | Weight: inverse | Error: 0.2501 | F1: 0.1486\n",
      "LR: 1.0 | L1: 0.1 | Iter: 2000 | Weight: log | Error: 0.2117 | F1: 0.1674\n",
      "LR: 1.0 | L1: 0.1 | Iter: 2000 | Weight: log | Error: 0.2117 | F1: 0.1674\n",
      "LR: 1.0 | L1: 0.1 | Iter: 3000 | Weight: uniform | Error: 0.3191 | F1: 0.0000\n",
      "LR: 1.0 | L1: 0.1 | Iter: 3000 | Weight: uniform | Error: 0.3191 | F1: 0.0000\n",
      "LR: 1.0 | L1: 0.1 | Iter: 3000 | Weight: balanced | Error: 0.6478 | F1: 0.4151\n",
      "LR: 1.0 | L1: 0.1 | Iter: 3000 | Weight: balanced | Error: 0.6478 | F1: 0.4151\n",
      "LR: 1.0 | L1: 0.1 | Iter: 3000 | Weight: inverse | Error: 0.2499 | F1: 0.3363\n",
      "LR: 1.0 | L1: 0.1 | Iter: 3000 | Weight: inverse | Error: 0.2499 | F1: 0.3363\n",
      "LR: 1.0 | L1: 0.1 | Iter: 3000 | Weight: log | Error: 0.2120 | F1: 0.1674\n",
      "LR: 1.0 | L1: 0.1 | Iter: 3000 | Weight: log | Error: 0.2120 | F1: 0.1674\n",
      "LR: 1.0 | L1: 0.1 | Iter: 5000 | Weight: uniform | Error: 0.3296 | F1: 0.0000\n",
      "LR: 1.0 | L1: 0.1 | Iter: 5000 | Weight: uniform | Error: 0.3296 | F1: 0.0000\n",
      "LR: 1.0 | L1: 0.1 | Iter: 5000 | Weight: balanced | Error: 0.6437 | F1: 0.4132\n",
      "LR: 1.0 | L1: 0.1 | Iter: 5000 | Weight: balanced | Error: 0.6437 | F1: 0.4132\n",
      "LR: 1.0 | L1: 0.1 | Iter: 5000 | Weight: inverse | Error: 0.2485 | F1: 0.1918\n",
      "LR: 1.0 | L1: 0.1 | Iter: 5000 | Weight: inverse | Error: 0.2485 | F1: 0.1918\n",
      "LR: 1.0 | L1: 0.1 | Iter: 5000 | Weight: log | Error: 0.2038 | F1: 0.1674\n",
      "Best F1 Score: 0.6050\n",
      "Best Parameters:\n",
      "  learning_rate: 0.5\n",
      "  lambda_l1: 0.0\n",
      "  num_iterations: 5000\n",
      "  weight_type: uniform\n",
      "LR: 1.0 | L1: 0.1 | Iter: 5000 | Weight: log | Error: 0.2038 | F1: 0.1674\n",
      "Best F1 Score: 0.6050\n",
      "Best Parameters:\n",
      "  learning_rate: 0.5\n",
      "  lambda_l1: 0.0\n",
      "  num_iterations: 5000\n",
      "  weight_type: uniform\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE GRID SEARCH WITH DYNAMIC CLASS WEIGHTS\n",
    "print(\"GRID SEARCH FOR OPTIMAL HYPERPARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define parameter grids (4 values each)\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'lambda_l1': [0.0, 0.001, 0.01, 0.1], \n",
    "    'num_iterations': [1000, 2000, 3000, 5000],\n",
    "    'weight_type': ['uniform', 'balanced', 'inverse', 'log']\n",
    "}\n",
    "\n",
    "print(f\"Grid Search Parameters:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# Initialize tracking variables\n",
    "best_f1 = 0\n",
    "best_params = {}\n",
    "results_log = []\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "# Grid search with cross-validation on validation set\n",
    "print(\"\\nStarting Grid Search...\")\n",
    "combination_count = 0\n",
    "\n",
    "for lr in param_grid['learning_rate']:\n",
    "    for lambda_l1 in param_grid['lambda_l1']:\n",
    "        for num_iter in param_grid['num_iterations']:\n",
    "            for weight_type in param_grid['weight_type']:\n",
    "                combination_count += 1\n",
    "                \n",
    "                try:\n",
    "                    # Compute dynamic class weights\n",
    "                    sample_weights, class_weights = compute_class_weights(y_train, weight_type)\n",
    "                    \n",
    "                    # Initialize weights\n",
    "                    np.random.seed(42)\n",
    "                    n_features = X_train.shape[1]\n",
    "                    w = np.random.normal(0, 0.01, n_features)\n",
    "                    b = 0.0\n",
    "                    \n",
    "                    # Train model\n",
    "                    w_trained, b_trained, cost_history = gradient_descent(\n",
    "                        X_train, y_train, w, b,\n",
    "                        learning_rate=lr, lambda_l1=lambda_l1, \n",
    "                        num_iterations=num_iter, sample_weights=sample_weights\n",
    "                    )\n",
    "                    \n",
    "                    # Evaluate on validation set (using default threshold 0.5 for fair comparison)\n",
    "                    y_pred_val, y_pred_proba_val = predict(X_val, w_trained, b_trained, threshold=0.5)\n",
    "                    metrics = evaluate_model(y_val, y_pred_val)\n",
    "                    \n",
    "                    # Log results\n",
    "                    result = {\n",
    "                        'learning_rate': lr,\n",
    "                        'lambda_l1': lambda_l1, \n",
    "                        'num_iterations': num_iter,\n",
    "                        'weight_type': weight_type,\n",
    "                        'f1_score': metrics['f1_score'],\n",
    "                        'accuracy': metrics['accuracy'],\n",
    "                        'precision': metrics['precision'],\n",
    "                        'recall': metrics['recall'],\n",
    "                        'final_cost': cost_history[-1] if cost_history else float('inf')\n",
    "                    }\n",
    "                    results_log.append(result)\n",
    "                    \n",
    "                    # Track best model\n",
    "                    if metrics['f1_score'] > best_f1:\n",
    "                        best_f1 = metrics['f1_score']\n",
    "                        best_params = {\n",
    "                            'learning_rate': lr,\n",
    "                            'lambda_l1': lambda_l1,\n",
    "                            'num_iterations': num_iter,\n",
    "                            'weight_type': weight_type,\n",
    "                            'weights': w_trained,\n",
    "                            'bias': b_trained,\n",
    "                            'metrics': metrics\n",
    "                        }\n",
    "                    \n",
    "                    # Simple progress output with error tracking\n",
    "                    final_cost = cost_history[-1] if cost_history else float('inf')\n",
    "                    print(f\"LR: {lr} | L1: {lambda_l1} | Iter: {num_iter} | Weight: {weight_type} | Error: {final_cost:.4f} | F1: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in combination {combination_count}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "print(f\"Best Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['weights', 'bias', 'metrics']:\n",
    "        print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55764f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING FINAL MODEL WITH BEST PARAMETERS\n",
      "=============================================\n",
      "Final Model Parameters:\n",
      "  Learning Rate: 0.5\n",
      "  L1 Regularization: 0.0\n",
      "  Iterations: 5000\n",
      "  Weight Type: uniform\n",
      "\n",
      "Final Class Weights (uniform):\n",
      "  Class 0 (Non-spam): 1.000\n",
      "  Class 1 (Spam): 1.000\n",
      "\n",
      "Final Training Complete!\n",
      "Final Cost: 0.1473\n",
      "Cost Reduction: 0.6935 -> 0.1473\n",
      "Convergence: Good\n",
      "\n",
      "Final Training Complete!\n",
      "Final Cost: 0.1473\n",
      "Cost Reduction: 0.6935 -> 0.1473\n",
      "Convergence: Good\n"
     ]
    }
   ],
   "source": [
    "# FINAL MODEL TRAINING WITH BEST PARAMETERS\n",
    "print(\"TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Use best parameters found from grid search\n",
    "BEST_LR = best_params['learning_rate']\n",
    "BEST_LAMBDA = best_params['lambda_l1'] \n",
    "BEST_ITER = best_params['num_iterations']\n",
    "BEST_WEIGHT_TYPE = best_params['weight_type']\n",
    "\n",
    "print(f\"Final Model Parameters:\")\n",
    "print(f\"  Learning Rate: {BEST_LR}\")\n",
    "print(f\"  L1 Regularization: {BEST_LAMBDA}\")\n",
    "print(f\"  Iterations: {BEST_ITER}\")\n",
    "print(f\"  Weight Type: {BEST_WEIGHT_TYPE}\")\n",
    "\n",
    "# Final training with best parameters\n",
    "sample_weights_final, class_weights_final = compute_class_weights(y_train, BEST_WEIGHT_TYPE)\n",
    "\n",
    "print(f\"\\nFinal Class Weights ({BEST_WEIGHT_TYPE}):\")\n",
    "print(f\"  Class 0 (Non-spam): {class_weights_final[0]:.3f}\")\n",
    "print(f\"  Class 1 (Spam): {class_weights_final[1]:.3f}\")\n",
    "\n",
    "# Train final model\n",
    "np.random.seed(42)\n",
    "n_features = X_train.shape[1]\n",
    "w_final = np.random.normal(0, 0.01, n_features)\n",
    "b_final = 0.0\n",
    "\n",
    "w_trained, b_trained, cost_history = gradient_descent(\n",
    "    X_train, y_train, w_final, b_final,\n",
    "    learning_rate=BEST_LR, lambda_l1=BEST_LAMBDA, \n",
    "    num_iterations=BEST_ITER, sample_weights=sample_weights_final\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Training Complete!\")\n",
    "print(f\"Final Cost: {cost_history[-1]:.4f}\")\n",
    "print(f\"Cost Reduction: {cost_history[0]:.4f} -> {cost_history[-1]:.4f}\")\n",
    "print(f\"Convergence: {'Good' if cost_history[-1] < cost_history[0] * 0.5 else 'Check'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "194e1996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING TRAINED MODEL ON VALIDATION DATA\n",
      "==================================================\n",
      "Log loss: 0.16237\n",
      "\n",
      " VALIDATION SET PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:  0.9374 (93.74%)\n",
      "Precision: 0.7148\n",
      "Recall:    0.5244\n",
      "F1 Score:  0.6050\n",
      "\n",
      "Confusion Matrix:\n",
      "  TP:  183 | FP:   73\n",
      "  FN:  166 | TN: 3398\n"
     ]
    }
   ],
   "source": [
    "# MODEL EVALUATION ON VALIDATION SET\n",
    "print(\"EVALUATING TRAINED MODEL ON VALIDATION DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# validation set predictions\n",
    "y_pred_val, y_pred_proba_val = predict(X_val, w_trained, b_trained)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "val_metrics = evaluate_model(y_val, y_pred_val)\n",
    "\n",
    "#check the log loss\n",
    "val_log_loss = log_loss(y_val, y_pred_proba_val)\n",
    "print(f\"Log loss: {val_log_loss:.5f}\")\n",
    "\n",
    "# Display results using our helper function\n",
    "print_results(val_metrics, \" VALIDATION SET PERFORMANCE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d2f2096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING TEST DATA FOR PREDICTIONS\n",
      "========================================\n",
      "Test data shape: (900, 6)\n",
      "Test data columns: ['message_id', 'num_links', 'num_words', 'has_offer', 'sender_score', 'all_caps']\n",
      "Message ID range: 20000 to 20899\n",
      "Test features shape: (900, 5)\n",
      "Test features sample:\n",
      "   num_links  num_words  has_offer  sender_score  all_caps\n",
      "0  -0.408058  -1.397873          0     -0.319948         0\n",
      "1  -1.227003   1.238106          0     -0.618016         0\n",
      "2  -1.227003   0.122144          0     -1.583159         0\n",
      "3  -1.227003   0.045181          0     -0.253475         0\n",
      "4   0.410887  -0.339633          1      1.344932         1\n",
      "Test data shape: (900, 6)\n",
      "Test data columns: ['message_id', 'num_links', 'num_words', 'has_offer', 'sender_score', 'all_caps']\n",
      "Message ID range: 20000 to 20899\n",
      "Test features shape: (900, 5)\n",
      "Test features sample:\n",
      "   num_links  num_words  has_offer  sender_score  all_caps\n",
      "0  -0.408058  -1.397873          0     -0.319948         0\n",
      "1  -1.227003   1.238106          0     -0.618016         0\n",
      "2  -1.227003   0.122144          0     -1.583159         0\n",
      "3  -1.227003   0.045181          0     -0.253475         0\n",
      "4   0.410887  -0.339633          1      1.344932         1\n"
     ]
    }
   ],
   "source": [
    "# LOAD AND PREPROCESS TEST DATA\n",
    "print(\"LOADING TEST DATA FOR PREDICTIONS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Test data columns: {list(test_data.columns)}\")\n",
    "\n",
    "# Extract message IDs for submission\n",
    "test_message_ids = test_data['message_id'].values\n",
    "print(f\"Message ID range: {test_message_ids.min()} to {test_message_ids.max()}\")\n",
    "\n",
    "# Prepare features (same as training)\n",
    "X_test = test_data[['num_links', 'num_words', 'has_offer', 'sender_score', 'all_caps']]\n",
    "\n",
    "# Apply same scaling as training data\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "print(f\"Test features shape: {X_test_scaled.shape}\")\n",
    "print(f\"Test features sample:\")\n",
    "print(X_test_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45c22126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING PREDICTIONS ON TEST DATA\n",
      "========================================\n",
      "Predictions generated for 900 samples\n",
      "Predicted spam count: 63\n",
      "Predicted non-spam count: 837\n",
      "Spam percentage: 7.0%\n",
      "\n",
      "Sample predictions:\n",
      "  Message 20000: NOT SPAM (confidence: 0.999)\n",
      "  Message 20001: NOT SPAM (confidence: 1.000)\n",
      "  Message 20002: NOT SPAM (confidence: 0.999)\n",
      "  Message 20003: NOT SPAM (confidence: 1.000)\n",
      "  Message 20004: SPAM (confidence: 0.751)\n"
     ]
    }
   ],
   "source": [
    "# MAKE PREDICTIONS ON TEST DATA\n",
    "print(\"GENERATING PREDICTIONS ON TEST DATA\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Use trained model to make predictions\n",
    "y_test_pred, y_test_proba = predict(X_test_scaled.values, w_trained, b_trained)\n",
    "\n",
    "print(f\"Predictions generated for {len(y_test_pred)} samples\")\n",
    "print(f\"Predicted spam count: {np.sum(y_test_pred)}\")\n",
    "print(f\"Predicted non-spam count: {np.sum(1 - y_test_pred)}\")\n",
    "print(f\"Spam percentage: {np.mean(y_test_pred)*100:.1f}%\")\n",
    "\n",
    "# Show some sample predictions\n",
    "print(f\"\\nSample predictions:\")\n",
    "for i in range(5):\n",
    "    spam_status = \"SPAM\" if y_test_pred[i] == 1 else \"NOT SPAM\"\n",
    "    confidence = y_test_proba[i] if y_test_pred[i] == 1 else (1 - y_test_proba[i])\n",
    "    print(f\"  Message {test_message_ids[i]}: {spam_status} (confidence: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c40d416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING SUBMISSION CSV FILE WITH SPAM PROBABILITIES\n",
      "=======================================================\n",
      "Submission file shape: (900, 2)\n",
      "Columns: ['message_id', 'is_spam']\n",
      "Probability range: 0.0001 to 0.9996\n",
      "\n",
      "First 5 rows (probabilities):\n",
      "   message_id  is_spam\n",
      "0       20000   0.0010\n",
      "1       20001   0.0003\n",
      "2       20002   0.0006\n",
      "3       20003   0.0003\n",
      "4       20004   0.7509\n",
      "\n",
      "Last 5 rows (probabilities):\n",
      "     message_id  is_spam\n",
      "895       20895   0.1816\n",
      "896       20896   0.0013\n",
      "897       20897   0.0582\n",
      "898       20898   0.1101\n",
      "899       20899   0.0015\n",
      "\n",
      "Probability distribution:\n",
      "  Mean probability: 0.0952\n",
      "  Median probability: 0.0049\n",
      "  High confidence spam (>0.8): 20\n",
      "  Low confidence non-spam (<0.2): 760\n",
      "\n",
      "Submission file saved as 'submission.csv' with probabilities!\n",
      "File contains 900 probability predictions\n",
      "Ready for submission!\n",
      "\n",
      "Submission file saved as 'submission.csv' with probabilities!\n",
      "File contains 900 probability predictions\n",
      "Ready for submission!\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING SUBMISSION CSV FILE WITH SPAM PROBABILITIES\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'message_id': test_message_ids,\n",
    "    'is_spam': y_test_proba  \n",
    "})\n",
    "\n",
    "submission_df['is_spam'] = submission_df['is_spam'].round(4)\n",
    "\n",
    "print(f\"Submission file shape: {submission_df.shape}\")\n",
    "print(f\"Columns: {list(submission_df.columns)}\")\n",
    "print(f\"Probability range: {submission_df['is_spam'].min():.4f} to {submission_df['is_spam'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows (probabilities):\")\n",
    "print(submission_df.head())\n",
    "\n",
    "print(f\"\\nLast 5 rows (probabilities):\")\n",
    "print(submission_df.tail())\n",
    "\n",
    "print(f\"\\nProbability distribution:\")\n",
    "print(f\"  Mean probability: {submission_df['is_spam'].mean():.4f}\")\n",
    "print(f\"  Median probability: {submission_df['is_spam'].median():.4f}\")\n",
    "print(f\"  High confidence spam (>0.8): {np.sum(submission_df['is_spam'] > 0.8)}\")\n",
    "print(f\"  Low confidence non-spam (<0.2): {np.sum(submission_df['is_spam'] < 0.2)}\")\n",
    "\n",
    "# Save to CSV file\n",
    "submission_filename = 'submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file saved as '{submission_filename}' with probabilities!\")\n",
    "print(f\"File contains {len(submission_df)} probability predictions\")\n",
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
